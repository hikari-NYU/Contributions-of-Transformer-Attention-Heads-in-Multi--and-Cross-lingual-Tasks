## Contributions of Transformer Attention Heads in Multi- and Cross-lingual Tasks

Code for ACL 2021 paper: Contributions of Transformer Attention Heads in Multi- and Cross-lingual Tasks.

#### Required packages:

```
torch==1.7.0
torchsummary==1.5.1
transformers=3.0.2
metrics==0.3.3
numpy==1.16.2
torchvision==0.4.2
scikit-learn==0.20.3
python-louvain==0.13
seqeval==0.0.12
tqdm=4.55.1
```

OR install with:

> pip install -r requirements.txt

### Run:

Please see details in each task README in corresponding task folder.

## Cite

If you use the dataset or the code, please cite this paper:

```
@inproceedings{ma-etal-2021-contributions,
  title={Contributions of Transformer Attention Heads in Multi- and Cross-lingual Tasks},
  author={Weicheng Ma, Kai Zhang, Renze Lou, Lili Wang, Soroush Vosoughi},
  booktitle={Proceedings of ACL-IJCNLP 2021},
  year={2021}
}
```

## Question

If you have any questions, feel free to contact `weicheng.ma.gr@dartmouth.edu` and  `drogozhang@gmail.com`.
